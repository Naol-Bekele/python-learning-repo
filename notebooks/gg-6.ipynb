{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Interacting With the Web üåê\n",
    "\n",
    "Web scraping is the process of collecting and parsing raw data from the Web. Many disciplines like data science, business intelligence, and investigative reporting benefit from collecting and analyzing data from websites.\n",
    "\n",
    "## What You'll Learn üìö\n",
    "- Scrape and parse text from websites\n",
    "- Use regular expressions for pattern matching\n",
    "- Work with HTML parsers like Beautiful Soup\n",
    "- Interact with web forms using MechanicalSoup\n",
    "- Handle real-time website interactions\n",
    "\n",
    "## Important Note ‚ö†Ô∏è\n",
    "Always check a website's Terms of Service before scraping. Some websites explicitly forbid automated data collection. Be respectful with request frequency to avoid overwhelming servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 Scrape and Parse Text From Websites\n",
    "\n",
    "### Basic Web Scraping with urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the codes and run it\n",
    "#from urllib.request import urlopen\n",
    "\n",
    "# Open a web page\n",
    "#url = \"http://olympus.realpython.org/profiles/aphrodite\"\n",
    "#page = urlopen(url)\n",
    "\n",
    "# Extract HTML content\n",
    "#html_bytes = page.read()\n",
    "#html = html_bytes.decode(\"utf-8\")\n",
    "\n",
    "#print(html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Text with String Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract title using string methods\n",
    "title_index = html.find(\"<title>\")\n",
    "start_index = title_index + len(\"<title>\")\n",
    "end_index = html.find(\"</title>\")\n",
    "title = html[start_index:end_index]\n",
    "\n",
    "print(\"Extracted title:\", title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 Regular Expressions Primer üîç\n",
    "\n",
    "Regular expressions are patterns used to search for text within strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern 'ab*c':\n",
      "ac: ['ac']\n",
      "abc: ['abc']\n",
      "abbbc: ['abbbc']\n",
      "abdc: []\n",
      "\n",
      "Case-insensitive 'ab*c':\n",
      "ABC: ['ABC']\n",
      "\n",
      "Pattern 'a.c':\n",
      "abc: ['abc']\n",
      "abbc: []\n",
      "ac: []\n",
      "\n",
      "Greedy substitution: Everything is ELEPHANTS.\n",
      "Non-greedy substitution: Everything is ELEPHANTS if it's in ELEPHANTS.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Basic pattern matching\n",
    "print(\"Pattern 'ab*c':\")\n",
    "print(\"ac:\", re.findall(\"ab*c\", \"ac\"))\n",
    "print(\"abc:\", re.findall(\"ab*c\", \"abc\"))\n",
    "print(\"abbbc:\", re.findall(\"ab*c\", \"abbbc\"))\n",
    "print(\"abdc:\", re.findall(\"ab*c\", \"abdc\"))\n",
    "\n",
    "# Case-insensitive matching\n",
    "print(\"\\nCase-insensitive 'ab*c':\")\n",
    "print(\"ABC:\", re.findall(\"ab*c\", \"ABC\", re.IGNORECASE))\n",
    "\n",
    "# Using wildcards\n",
    "print(\"\\nPattern 'a.c':\")\n",
    "print(\"abc:\", re.findall(\"a.c\", \"abc\"))\n",
    "print(\"abbc:\", re.findall(\"a.c\", \"abbc\"))\n",
    "print(\"ac:\", re.findall(\"a.c\", \"ac\"))\n",
    "\n",
    "# Greedy vs non-greedy matching\n",
    "string = \"Everything is <replaced> if it's in <tags>.\"\n",
    "greedy = re.sub(\"<.*>\", \"ELEPHANTS\", string)\n",
    "nongreedy = re.sub(\"<.*?>\", \"ELEPHANTS\", string)\n",
    "\n",
    "print(\"\\nGreedy substitution:\", greedy)\n",
    "print(\"Non-greedy substitution:\", nongreedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Text with Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import re\n",
    "\n",
    "url = \"http://olympus.realpython.org/profiles/dionysus\"\n",
    "page = urlopen(url)\n",
    "html = page.read().decode(\"utf-8\")\n",
    "\n",
    "# Extract title using regex\n",
    "pattern = \"<title.*?>.*?</title.*?>\"\n",
    "match_results = re.search(pattern, html, re.IGNORECASE)\n",
    "title = match_results.group()\n",
    "title = re.sub(\"<.*?>\", \"\", title)  # Remove HTML tags\n",
    "\n",
    "print(\"Extracted title:\", title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 Using HTML Parsers üß∞\n",
    "\n",
    "### Beautiful Soup Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install beautifulsoup4\n",
    "#uncomment all\n",
    "#from bs4 import BeautifulSoup\n",
    "#from urllib.request import urlopen\n",
    "\n",
    "#url = \"http://olympus.realpython.org/profiles/dionysus\"\n",
    "#page = urlopen(url)\n",
    "#html = page.read().decode(\"utf-8\")\n",
    "#soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Extract all text\n",
    "#print(\"All text:\\n\", soup.get_text())\n",
    "\n",
    "# Extract specific elements\n",
    "#print(\"\\nTitle:\", soup.title)\n",
    "#print(\"Title string:\", soup.title.string)\n",
    "\n",
    "# Find all images\n",
    "#images = soup.find_all(\"img\")\n",
    "#print(\"\\nImages:\", images)\n",
    "\n",
    "# Extract image sources\n",
    "#for img in images:\n",
    "#print(\"Image source:\", img[\"src\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4 Interacting With Forms üìù\n",
    "\n",
    "### MechanicalSoup Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install MechanicalSoup\n",
    "import mechanicalsoup\n",
    "\n",
    "# Create a browser object\n",
    "browser = mechanicalsoup.Browser()\n",
    "\n",
    "# Request a page\n",
    "url = \"http://olympus.realpython.org/login\"\n",
    "login_page = browser.get(url)\n",
    "print(\"Status code:\", login_page.status_code)\n",
    "print(\"Login page HTML:\\n\", login_page.soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form Submission Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the form\n",
    "login_html = login_page.soup\n",
    "form = login_html.select(\"form\")[0]\n",
    "\n",
    "# Fill in credentials\n",
    "form.select(\"input\")[0][\"value\"] = \"zeus\"\n",
    "form.select(\"input\")[1][\"value\"] = \"ThunderDude\"\n",
    "\n",
    "# Submit form\n",
    "profiles_page = browser.submit(form, login_page.url)\n",
    "\n",
    "# Check if login was successful\n",
    "print(\"Current URL:\", profiles_page.url)\n",
    "if profiles_page.url == \"http://olympus.realpython.org/profiles\":\n",
    "    print(\"Login successful!\")\n",
    "    \n",
    "# Extract profile links\n",
    "links = profiles_page.soup.select(\"a\")\n",
    "base_url = \"http://olympus.realpython.org\"\n",
    "print(\"\\nAvailable profiles:\")\n",
    "for link in links:\n",
    "    address = base_url + link[\"href\"]\n",
    "    text = link.text\n",
    "    print(f\"{text}: {address}\")\n",
    "else:\n",
    "    print(\"Login failed.\")\n",
    "    # Check for error message\n",
    "    error = profiles_page.soup.find(text=\"Wrong username or password!\")\n",
    "    if error:\n",
    "        print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.5 Real-Time Interaction ‚è±Ô∏è\n",
    "\n",
    "### Monitoring a Changing Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run\n",
    "#import time\n",
    "#import mechanicalsoup\n",
    "\n",
    "#browser = mechanicalsoup.Browser()\n",
    "\n",
    "#for i in range(4):\n",
    "    # Get the page\n",
    "    #page = browser.get(\"http://olympus.realpython.org/dice\")\n",
    "    \n",
    "    # Extract the result\n",
    "    #tag = page.soup.select(\"#result\")[0]\n",
    "    #result = tag.text\n",
    "    \n",
    "    # Extract the time\n",
    "    #time_tag = page.soup.select(\"p\")[1]  # Assuming time is in the second paragraph\n",
    "    #time_text = time_tag.text\n",
    "    \n",
    "    #print(f\"Roll {i+1}: {result} at {time_text}\")\n",
    "    \n",
    "    # Wait before next request (if not the last one)\n",
    "    #if i < 3:\n",
    "        #time.sleep(5)  # Wait 5 seconds between requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises üí™\n",
    "\n",
    "### Exercise 1: Basic Scraping\n",
    "Scrape the profile page of Poseidon from http://olympus.realpython.org/profiles/poseidon and extract:\n",
    "1. The page title\n",
    "2. All text content\n",
    "3. The favorite animal mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Form Handling\n",
    "Create a script that:\n",
    "1. Attempts to log in to http://olympus.realpython.org/login with incorrect credentials\n",
    "2. Verifies that the login failed by checking for the error message\n",
    "3. Then logs in with the correct credentials (zeus/ThunderDude)\n",
    "4. Prints the title of the resulting page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Exercise: Data Aggregation\n",
    "Write a program that:\n",
    "1. Logs in to the Olympus site\n",
    "2. Visits each profile page (Aphrodite, Poseidon, Dionysus)\n",
    "3. Collects the following data from each profile:\n",
    "   - Name\n",
    "   - Hometown\n",
    "   - Favorite animal\n",
    "   - Favorite color\n",
    "4. Stores the data in a dictionary\n",
    "5. Prints the collected data in a formatted way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for the Challenge Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources üìö\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [MechanicalSoup Documentation](https://mechanicalsoup.readthedocs.io/)\n",
    "- [Regular Expressions Guide](https://docs.python.org/3/howto/regex.html)\n",
    "- [Web Scraping Best Practices](https://www.scrapehero.com/web-scraping-best-practices/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
